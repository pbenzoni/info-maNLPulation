{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# String Theory: Unfolding additional dimensions beyond a 3∆-Space Approaches to identifying Copy-Pasta, Rewording, and Translation in Information Manipulation\n",
    "TKTKTKTK In an era where social media plays a pivotal role in shaping public opinion and discourse, the manipulation of information has emerged as a significant challenge. The proliferation of sophisticated techniques, such as copy-pasting, rewording, and translating messages, has enabled malicious actors to conduct large-scale information manipulation campaigns. These campaigns aim to influence opinions, spread disinformation, and evade detection by platform moderators. The original study by Richard et al. (2023) introduced the “3∆-space duplicate methodology,” a novel approach to identifying these manipulation techniques by quantifying semantic, grapheme, and language proximities within messages.\n",
    "\n",
    "While the 3∆-space methodology has proven effective in detecting coordinated inauthentic behavior, there remain opportunities for enhancement. This paper seeks to build upon the foundational work of Richard et al., proposing refined techniques and advanced algorithms to improve the accuracy and efficiency of detecting manipulated textual content. By leveraging state-of-the-art machine learning models and innovative computational methods, we aim to address the limitations of the original approach and provide a more robust framework for identifying information manipulation on social media platforms.\n",
    "\n",
    "Our enhanced methodology focuses on three primary areas of improvement: semantic proximity analysis, grapheme distance computation, and language differentiation. We introduce advanced sentence embeddings and refined distance metrics to better capture the nuances of textual manipulation. Additionally, we explore the integration of new tools and datasets to validate our approach, ensuring its applicability across diverse contexts and languages.\n",
    "\n",
    "The significance of this research lies in its potential to advance the field of information integrity and security. By improving detection capabilities, we can better identify and mitigate the impact of coordinated inauthentic behavior, thereby safeguarding the integrity of public discourse. This paper not only contributes to the academic understanding of information manipulation techniques but also provides practical solutions for social media platforms and policymakers to enhance their defense mechanisms against disinformation campaigns.\n",
    "\n",
    "In the following sections, we will detail the proposed improvements to the 3∆-space methodology, present our experimental validation using synthetic and real-world datasets, and discuss the implications of our findings for future research and practical applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Limitations of the 3∆ Approach \n",
    "\n",
    "### The Twitter Transparency Dataset\n",
    "While the Twitter dataset related to Venezuelan actors, released by Twitter Transparency in 2021, offers valuable insights into information manipulation tactics, it presents several limitations that could impact the robustness and generalizability of the findings. Firstly, the dataset's geographical focus on Venezuelan actors may limit the applicability of the results to other regions or contexts. Information manipulation techniques can vary significantly based on cultural, political, and social factors; therefore, the insights derived from this dataset might not fully capture the nuances of similar campaigns conducted in different geopolitical environments.\n",
    "\n",
    "Secondly, the dataset is predominantly in Spanish (86.2%), with only marginal representation of other languages such as English (1.5%) and Portuguese (1.1%)​(2312.17338v1)​. This language limitation poses a challenge for developing and validating detection methodologies that are effective across multiple languages. Since the detection of translated content is a key component of identifying information manipulation, the lack of linguistic diversity in the dataset may hinder the development of robust, cross-linguistic detection algorithms.\n",
    "\n",
    "Finally, the dataset's temporal scope and length restrictions also pose limitations. The analysis focuses on tweets from January to June 2021, which might not provide a comprehensive view of longer-term manipulation strategies. Furthermore, Twitter's inherent character limit—140 characters for tweets prior to 2017 and 280 characters thereafter—imposes constraints on the length and complexity of the messages analyzed. Shorter text lengths may result in reduced semantic richness, making it challenging to accurately detect nuanced manipulations such as subtle rewordings or sophisticated translations.\n",
    "\n",
    "\n",
    "### Platform specific adjustments (out of scope)\n",
    "\n",
    "The original paper, despite using a single platform's dataset, makes no efforts to adjust its approach to the unique data provided by that platform — namely usernames hashtags, and data about the posting users. While this is out of the scope of this paper, any truly comprehensive quantative approach to \"identifying Copy-Pasta, Rewording, and Translation in Information Manipulation\" should use this information as part of a model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  ∆1: Grapheme distance\n",
    "\n",
    "#### Using NLP data cleaning techniques  \n",
    "The described approach, while comprehensive in its use of  grapheme distance algorithms for detecting Copy-Pasta, has a few limitations that can be addressed using basic NLP data cleaning techniques. The primary criticisms revolve around lack of text normalization, and the potential for noise in the data that could be mitigated through preprocessing steps.\n",
    "- Presence of Stopwords: Stopwords are often irrelevant to the meaning of a text but can affect the grapheme distance calculations. Their presence can create false positives in the detection of Copy-Pasta and rewording.\n",
    "- Lemmatization and Stemming: The current approach does not leverage lemmatization or stemming, which can help in reducing words to their base or root form. This omission means that words like \"running\" and \"ran\" are treated as different, increasing grapheme distances unnecessarily. \n",
    "\n",
    "#### Changing granularity \n",
    "Using letters as tokens rather than words in grapheme distance calculations has several limitations that can negatively impact the accuracy and effectiveness of detecting textual manipulations. \n",
    "- Treating letters as the unit of measure results in a very fine-grained analysis. This granularity can lead to an overemphasis on minor, superficial changes such as typos, small edits, or formatting differences, which may not significantly alter the overall meaning of the text. For instance, a single character change due to a typo would result in a high grapheme distance, potentially misclassifying the text as reworded or manipulated.\n",
    "- Letter-based analysis ignores the context provided by whole words. Words carry meaning and contextual information that individual letters do not. For example, the difference between \"cat\" and \"bat\" is significant in a word context but might be overly emphasized in a letter-based distance metric.\n",
    "- Textual noise, such as typographical errors or inserted special characters, can have a large impact on letter-based grapheme distances. This sensitivity can obscure genuine content similarities or differences.\n",
    "- Calculating grapheme distances based on individual letters can be computationally expensive, especially for longer texts. The number of comparisons needed increases significantly with text length, leading to inefficiencies and potentially longer processing times.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Semantic Clustering\n",
    "No attempt is made by the exist methodology to generate semantic clusters that can be used to detect suspect new terms "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  ∆2: Semantic  distance\n",
    "\n",
    "#### Comaparing USE With BERT, SBERT, RoBERTa, GPT3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  ∆3: Translation\n",
    "\n",
    "#### Comaparing USE With BERT, SBERT, RoBERTa, GPT3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  ∆5: Classifiers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries, Constants, and preloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\benzo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\benzo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\benzo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "file_path_naive = \"C:\\\\Users\\\\benzo\\\\repo\\\\nlp-project\\\\data\\\\combined_dataset.csv\"\n",
    "file_path_detailed = \"C:\\\\Users\\\\benzo\\\\repo\\\\nlp-project\\\\data\\\\combined_dataset_detailed.csv\"\n",
    "file_path_info_ops = \"C:\\\\Users\\\\benzo\\\\repo\\\\nlp-project\\\\data\\\\combined_info_ops_dataset_slim.csv\"\n",
    "RAND_SEED = 417\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvements — an expanded dataset: Adding Context, Sources, Campaigns, and Languages\n",
    "\n",
    "All of the datasets listed from June 2020 through 2021 are listed, except the following, for the noted reasons\n",
    "- Armenia (February 2021) - low number of accounts and tweets\n",
    "- Russia (September 2020) - low number of accounts \n",
    "- Saudi Arabia (September 2020) - Low number of accounts and tweets \n",
    "- Turkey - Overwhelming large number of posts (5 GB) \n",
    "\n",
    "A comparison set of tweets is sourced from the following two datasets: \n",
    "English Tweets of 2022 - https://www.kaggle.com/datasets/amirhosseinnaghshzan/twitter-2022\n",
    "1.6 million random tweets - https://www.kaggle.com/datasets/i191796majid/tweets\n",
    "\n",
    "\n",
    "Datasets and the ETL process can be found in the ./data_transform.py. To summarize transformations made: The combine_comparison_datasets function loads three separate CSV files, concatenates their tweet text columns into a single DataFrame, and filters out short tweets. The combined DataFrame is then saved to a specified output file. Similarly, the combine_info_ops_datasets function processes multiple CSV files in a specified directory, appending the source of each tweet based on the filename and filtering out retweets and tweets with undefined or short text. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvements - do we need to bother with a quantative approach?\n",
    "\n",
    "Before creating a \"quantitative approach to detecting Copy-pasta, Rewording, and Translation on Social Media\",  *Unmasking information manipulation* fails to address the fundamental premise: are information operations linguistically distinctitive from authentic content? \n",
    "\n",
    "To explore this question, we can implement a series of machine learning classifiers to analyze tweets and determine if there's a distinctive linguistic pattern between information operations and authentic content. The following code demonstrates how to load, preprocess, and classify a dataset of tweets to identify these patterns. It begins by taking a some 'naive' approach, assuming all information operations have something in common, then we split out by sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\benzo\\AppData\\Local\\Temp\\ipykernel_9952\\3218998661.py:36: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_naive = pd.read_csv(file_path_naive)\n",
      "C:\\Users\\benzo\\AppData\\Local\\Temp\\ipykernel_9952\\3218998661.py:37: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_detailed = pd.read_csv(file_path_detailed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive dataset\n",
      "Classifier: MultinomialNB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  comparison       0.94      1.00      0.97     94159\n",
      "    info_ops       0.98      0.61      0.75     15407\n",
      "\n",
      "    accuracy                           0.94    109566\n",
      "   macro avg       0.96      0.80      0.86    109566\n",
      "weighted avg       0.95      0.94      0.94    109566\n",
      "\n",
      "Classifier: MultinomialNB_tightfit\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  comparison       0.97      0.98      0.98     94159\n",
      "    info_ops       0.89      0.84      0.87     15407\n",
      "\n",
      "    accuracy                           0.96    109566\n",
      "   macro avg       0.93      0.91      0.92    109566\n",
      "weighted avg       0.96      0.96      0.96    109566\n",
      "\n",
      "Classifier: LogisticRegression_HighCon\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  comparison       0.96      1.00      0.98     94159\n",
      "    info_ops       0.96      0.75      0.84     15407\n",
      "\n",
      "    accuracy                           0.96    109566\n",
      "   macro avg       0.96      0.87      0.91    109566\n",
      "weighted avg       0.96      0.96      0.96    109566\n",
      "\n",
      "Classifier: LogisticRegression_LowCon\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  comparison       0.95      1.00      0.97     94159\n",
      "    info_ops       0.96      0.71      0.82     15407\n",
      "\n",
      "    accuracy                           0.96    109566\n",
      "   macro avg       0.96      0.85      0.90    109566\n",
      "weighted avg       0.96      0.96      0.95    109566\n",
      "\n",
      "Classifier: RandomForest_Sparse\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\benzo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\benzo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\benzo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  comparison       0.86      1.00      0.92     94159\n",
      "    info_ops       0.00      0.00      0.00     15407\n",
      "\n",
      "    accuracy                           0.86    109566\n",
      "   macro avg       0.43      0.50      0.46    109566\n",
      "weighted avg       0.74      0.86      0.79    109566\n",
      "\n",
      "Classifier: RandomForest_Deep\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  comparison       0.90      1.00      0.95     94159\n",
      "    info_ops       1.00      0.30      0.46     15407\n",
      "\n",
      "    accuracy                           0.90    109566\n",
      "   macro avg       0.95      0.65      0.70    109566\n",
      "weighted avg       0.91      0.90      0.88    109566\n",
      "\n",
      "Detailed dataset\n",
      "Classifier: MultinomialNB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\benzo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\benzo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\benzo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        CNCC       0.00      0.00      0.00       129\n",
      "        CNHU       1.00      0.22      0.36       747\n",
      "         GRU       0.98      0.09      0.16       578\n",
      "         IRA       1.00      0.01      0.01      1133\n",
      "         REA       0.00      0.00      0.00       329\n",
      "         RNA       0.00      0.00      0.00       497\n",
      "   Venezuela       0.00      0.00      0.00       119\n",
      "       china       1.00      0.00      0.01       484\n",
      "  comparison       0.90      1.00      0.95     93893\n",
      "        iran       0.98      0.20      0.33      3485\n",
      "      russia       1.00      0.00      0.00       991\n",
      "      uganda       0.99      0.57      0.72      7148\n",
      "\n",
      "    accuracy                           0.90    109533\n",
      "   macro avg       0.65      0.17      0.21    109533\n",
      "weighted avg       0.90      0.90      0.87    109533\n",
      "\n",
      "Classifier: MultinomialNB_tightfit\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        CNCC       1.00      0.09      0.16       129\n",
      "        CNHU       0.99      0.61      0.76       747\n",
      "         GRU       0.91      0.72      0.81       578\n",
      "         IRA       0.86      0.43      0.57      1133\n",
      "         REA       0.98      0.15      0.26       329\n",
      "         RNA       0.97      0.17      0.29       497\n",
      "   Venezuela       1.00      0.06      0.11       119\n",
      "       china       0.92      0.12      0.21       484\n",
      "  comparison       0.96      0.99      0.97     93893\n",
      "        iran       0.74      0.75      0.75      3485\n",
      "      russia       0.67      0.26      0.38       991\n",
      "      uganda       0.92      0.89      0.91      7148\n",
      "\n",
      "    accuracy                           0.95    109533\n",
      "   macro avg       0.91      0.44      0.51    109533\n",
      "weighted avg       0.95      0.95      0.94    109533\n",
      "\n",
      "Classifier: LogisticRegression_HighCon\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        CNCC       1.00      0.09      0.16       129\n",
      "        CNHU       0.99      0.67      0.80       747\n",
      "         GRU       0.92      0.84      0.88       578\n",
      "         IRA       0.88      0.60      0.72      1133\n",
      "         REA       0.87      0.48      0.62       329\n",
      "         RNA       0.89      0.45      0.60       497\n",
      "   Venezuela       1.00      0.25      0.40       119\n",
      "       china       0.99      0.19      0.31       484\n",
      "  comparison       0.95      1.00      0.98     93893\n",
      "        iran       0.91      0.71      0.80      3485\n",
      "      russia       0.94      0.32      0.48       991\n",
      "      uganda       0.99      0.82      0.90      7148\n",
      "\n",
      "    accuracy                           0.95    109533\n",
      "   macro avg       0.94      0.53      0.64    109533\n",
      "weighted avg       0.95      0.95      0.95    109533\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\benzo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: LogisticRegression_LowCon\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        CNCC       1.00      0.02      0.05       129\n",
      "        CNHU       0.99      0.63      0.77       747\n",
      "         GRU       0.91      0.81      0.86       578\n",
      "         IRA       0.87      0.55      0.67      1133\n",
      "         REA       0.87      0.35      0.50       329\n",
      "         RNA       0.89      0.38      0.53       497\n",
      "   Venezuela       1.00      0.22      0.36       119\n",
      "       china       1.00      0.15      0.26       484\n",
      "  comparison       0.95      1.00      0.97     93893\n",
      "        iran       0.91      0.67      0.77      3485\n",
      "      russia       0.93      0.26      0.40       991\n",
      "      uganda       0.99      0.79      0.88      7148\n",
      "\n",
      "    accuracy                           0.95    109533\n",
      "   macro avg       0.94      0.48      0.58    109533\n",
      "weighted avg       0.95      0.95      0.94    109533\n",
      "\n",
      "Classifier: RandomForest_Sparse\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\benzo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\benzo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\benzo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        CNCC       0.00      0.00      0.00       129\n",
      "        CNHU       0.00      0.00      0.00       747\n",
      "         GRU       0.00      0.00      0.00       578\n",
      "         IRA       0.00      0.00      0.00      1133\n",
      "         REA       0.00      0.00      0.00       329\n",
      "         RNA       0.00      0.00      0.00       497\n",
      "   Venezuela       0.00      0.00      0.00       119\n",
      "       china       0.00      0.00      0.00       484\n",
      "  comparison       0.86      1.00      0.92     93893\n",
      "        iran       0.00      0.00      0.00      3485\n",
      "      russia       0.00      0.00      0.00       991\n",
      "      uganda       0.00      0.00      0.00      7148\n",
      "\n",
      "    accuracy                           0.86    109533\n",
      "   macro avg       0.07      0.08      0.08    109533\n",
      "weighted avg       0.73      0.86      0.79    109533\n",
      "\n",
      "Classifier: RandomForest_Deep\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\benzo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\benzo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        CNCC       0.00      0.00      0.00       129\n",
      "        CNHU       1.00      0.21      0.34       747\n",
      "         GRU       0.99      0.25      0.40       578\n",
      "         IRA       1.00      0.01      0.02      1133\n",
      "         REA       0.00      0.00      0.00       329\n",
      "         RNA       1.00      0.01      0.03       497\n",
      "   Venezuela       1.00      0.03      0.07       119\n",
      "       china       0.00      0.00      0.00       484\n",
      "  comparison       0.88      1.00      0.94     93893\n",
      "        iran       1.00      0.18      0.31      3485\n",
      "      russia       1.00      0.02      0.04       991\n",
      "      uganda       1.00      0.32      0.49      7148\n",
      "\n",
      "    accuracy                           0.89    109533\n",
      "   macro avg       0.74      0.17      0.22    109533\n",
      "weighted avg       0.89      0.89      0.85    109533\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\benzo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def try_out_models(df):\n",
    "    # Sample the data to speed up training, comment out if you want to use the full dataset. \n",
    "    df = df.sample(frac=0.25, replace=True, random_state=RAND_SEED)\n",
    "\n",
    "    # Minimally preprocess the data \n",
    "    X = df['tweet_text']\n",
    "    y = df['source']\n",
    "\n",
    "    # Vectorize the text data\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X_vectorized = vectorizer.fit_transform(X)\n",
    "\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_vectorized, y, test_size=0.2, random_state=RAND_SEED)\n",
    "\n",
    "    # Define classifiers, tweaking hyperparameters appropriately\n",
    "    classifiers = {\n",
    "        \"MultinomialNB\": MultinomialNB(),\n",
    "        \"MultinomialNB_tightfit\": MultinomialNB(alpha=0.1),\n",
    "        \"LogisticRegression_HighCon\": LogisticRegression(max_iter=1000),\n",
    "        \"LogisticRegression_LowCon\": LogisticRegression(max_iter=200, C=0.5),\n",
    "        #\"SVC_linear\": SVC(kernel='linear'), # takes too long to train, may revisit later\n",
    "        #\"SVC_rbf\": SVC(kernel='rbf'), # takes too long to train, may revisit later\n",
    "        \"RandomForest_Sparse\": RandomForestClassifier(max_depth=10, random_state=RAND_SEED),\n",
    "        \"RandomForest_Deep\": RandomForestClassifier(max_depth=100, random_state=RAND_SEED)\n",
    "\n",
    "    }\n",
    "\n",
    "    # Train and evaluate classifiers\n",
    "    for name, clf in classifiers.items():\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        print(f\"Classifier: {name}\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "df_naive = pd.read_csv(file_path_naive)\n",
    "df_detailed = pd.read_csv(file_path_detailed)\n",
    "\n",
    "# Load the naive dataset\n",
    "print(\"Naive dataset\")\n",
    "try_out_models(df_naive)\n",
    "\n",
    "# Repeat the process with detailed source samples\n",
    "print(\"Detailed dataset\")\n",
    "try_out_models(df_detailed)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis - \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvements - Grapheme \n",
    "\n",
    "### Additional Preprocessing NLP CLeaning:\n",
    "Stopword Removal:\n",
    "Removing stopwords before calculating grapheme distances can reduce noise and focus the analysis on meaningful content. This can help in more accurately identifying Copy-Pasta and rewording by eliminating common but insignificant words.\n",
    "\n",
    "Lemmatization and Stemming:\n",
    "Applying lemmatization or stemming can normalize words to their root forms, reducing the variability in the text. For instance, \"running\" and \"ran\" would both be reduced to \"run,\" making the grapheme distance more reflective of actual content changes rather than superficial differences.\n",
    "\n",
    "Consistent Spacing:\n",
    "Normalizing whitespace (e.g., converting multiple spaces to a single space) can prevent spacing differences from affecting the grapheme distance. This step can help in accurately categorizing texts that have been slightly altered in terms of spacing.\n",
    "\n",
    "### Decreasing Granularity \n",
    "Reduced Noise Sensitivity:\n",
    "Treating words as tokens can mitigate the impact of minor textual noise. Since words are larger units, small alterations (like a single-character change) will have a reduced effect on the overall distance calculation, leading to more stable and reliable results.\n",
    "\n",
    "Improved Context Handling:\n",
    "Words provide context that letters do not. Using words as tokens can help maintain the contextual integrity of the text, allowing for more accurate comparisons. For example, \"breaking news\" and \"urgent news\" share a contextual meaning that would be lost in a letter-based analysis.\n",
    "\n",
    "Efficient Computation:\n",
    "Word-based distance metrics can be more computationally efficient for longer texts. Instead of comparing every single letter, the algorithm can focus on comparing words, which can reduce the computational complexity and improve processing speed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preprocessing functions\n",
    "\n",
    "# Language-specific stopwords,  ru, es, en, zh, ar, fr are supported by NLTK and have > 10K results \n",
    "stop_words = {\n",
    "    'en': set(stopwords.words('english')),\n",
    "    'ru': set(stopwords.words('russian')),\n",
    "    'es': set(stopwords.words('spanish')),\n",
    "    'zh': set(stopwords.words('chinese')),\n",
    "    'fr': set(stopwords.words('french')),\n",
    "    'ar': set(stopwords.words('arabic'))\n",
    "    #'in': set(stopwords.words('indonesian'))\n",
    "}\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmers = {\n",
    "    'en': SnowballStemmer('english'),\n",
    "    'ru': SnowballStemmer('russian'),\n",
    "    'es': SnowballStemmer('spanish'),\n",
    "    'zh': None,  # Chinese does not use stemming in the same way\n",
    "    'fr': SnowballStemmer('french'),\n",
    "    'ar': SnowballStemmer('arabic')\n",
    "}\n",
    "\n",
    "def lower_and_remove_nonlang(text):\n",
    "    # Lowercasing\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    \n",
    "    # Remove non-language characters (punctuation, emojis, etc.)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    return text\n",
    "    \n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text, lang='en'):\n",
    "    # Lowercasing and removing non-language characters\n",
    "    text = lower_and_remove_nonlang(text)\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stop_words.get(lang, set())]\n",
    "    \n",
    "    # Stemming/Lemmatization\n",
    "    if lang == 'en':\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    else:\n",
    "        stemmer = stemmers.get(lang)\n",
    "        if stemmer:\n",
    "            tokens = [stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    # Consistent spacing (normalize whitespace)\n",
    "    text = ' '.join(tokens)\n",
    "        \n",
    "    return text\n",
    "\n",
    "\n",
    "df_info_ops = pd.read_csv(file_path_info_ops)\n",
    "df_info_ops['tweet_text_ppp'] = df_info_ops.apply(lambda row: lower_and_remove_nonlang(row['tweet_text']), axis=1)\n",
    "\n",
    "# Apply preprocessing to both datasets\n",
    "df_info_ops['tweet_text_fpp'] = df_info_ops.apply(lambda row: preprocess_text(row['tweet_text'], row['tweet_language']), axis=1)\n",
    "\n",
    "# Save the preprocessed datasets\n",
    "df_info_ops.to_csv(file_path_info_ops.replace('.csv', '_preprocessed.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing Grapheme distance — comparing methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "china\n",
      "CNCC\n",
      "CNHU\n",
      "GRU\n",
      "iran\n",
      "IRA\n",
      "MX\n",
      "REA\n",
      "RNA\n",
      "russia\n",
      "Tanzania\n",
      "thailand\n",
      "uganda\n",
      "Venezuela\n",
      "Jaccard (fully preprocessed): 0.06\n",
      "Dice (fully preprocessed): 0.03\n",
      "Levenshtein(fully preprocessed) : 0.31\n",
      "Jaccard (partially preprocessed): 0.08\n",
      "Dice (partially preprocessed): 0.03\n",
      "Levenshtein (partially preprocessed): 0.33\n",
      "\n",
      "Jaccard (fully preprocessed): 0.16\n",
      "Dice (fully preprocessed): 0.09\n",
      "Levenshtein(fully preprocessed) : 0.40\n",
      "Jaccard (partially preprocessed): 0.16\n",
      "Dice (partially preprocessed): 0.10\n",
      "Levenshtein (partially preprocessed): 0.40\n",
      "\n",
      "Jaccard (fully preprocessed): 0.03\n",
      "Dice (fully preprocessed): 0.01\n",
      "Levenshtein(fully preprocessed) : 0.30\n",
      "Jaccard (partially preprocessed): 0.06\n",
      "Dice (partially preprocessed): 0.01\n",
      "Levenshtein (partially preprocessed): 0.32\n",
      "\n",
      "Jaccard (fully preprocessed): 0.06\n",
      "Dice (fully preprocessed): 0.01\n",
      "Levenshtein(fully preprocessed) : 0.29\n",
      "Jaccard (partially preprocessed): 0.12\n",
      "Dice (partially preprocessed): 0.04\n",
      "Levenshtein (partially preprocessed): 0.32\n",
      "\n",
      "Jaccard (fully preprocessed): 0.43\n",
      "Dice (fully preprocessed): 0.47\n",
      "Levenshtein(fully preprocessed) : 0.63\n",
      "Jaccard (partially preprocessed): 0.47\n",
      "Dice (partially preprocessed): 0.53\n",
      "Levenshtein (partially preprocessed): 0.64\n",
      "\n",
      "Jaccard (fully preprocessed): 0.06\n",
      "Dice (fully preprocessed): 0.01\n",
      "Levenshtein(fully preprocessed) : 0.34\n",
      "Jaccard (partially preprocessed): 0.08\n",
      "Dice (partially preprocessed): 0.01\n",
      "Levenshtein (partially preprocessed): 0.33\n",
      "\n",
      "Jaccard (fully preprocessed): 0.47\n",
      "Dice (fully preprocessed): 0.33\n",
      "Levenshtein(fully preprocessed) : 0.59\n",
      "Jaccard (partially preprocessed): 0.47\n",
      "Dice (partially preprocessed): 0.33\n",
      "Levenshtein (partially preprocessed): 0.59\n",
      "\n",
      "Jaccard (fully preprocessed): 0.05\n",
      "Dice (fully preprocessed): 0.02\n",
      "Levenshtein(fully preprocessed) : 0.30\n",
      "Jaccard (partially preprocessed): 0.09\n",
      "Dice (partially preprocessed): 0.03\n",
      "Levenshtein (partially preprocessed): 0.32\n",
      "\n",
      "Jaccard (fully preprocessed): 0.05\n",
      "Dice (fully preprocessed): 0.01\n",
      "Levenshtein(fully preprocessed) : 0.32\n",
      "Jaccard (partially preprocessed): 0.09\n",
      "Dice (partially preprocessed): 0.03\n",
      "Levenshtein (partially preprocessed): 0.32\n",
      "\n",
      "Jaccard (fully preprocessed): 0.03\n",
      "Dice (fully preprocessed): 0.01\n",
      "Levenshtein(fully preprocessed) : 0.26\n",
      "Jaccard (partially preprocessed): 0.07\n",
      "Dice (partially preprocessed): 0.02\n",
      "Levenshtein (partially preprocessed): 0.33\n",
      "\n",
      "Jaccard (fully preprocessed): 0.26\n",
      "Dice (fully preprocessed): 0.15\n",
      "Levenshtein(fully preprocessed) : 0.46\n",
      "Jaccard (partially preprocessed): 0.28\n",
      "Dice (partially preprocessed): 0.22\n",
      "Levenshtein (partially preprocessed): 0.47\n",
      "\n",
      "Jaccard (fully preprocessed): 0.24\n",
      "Dice (fully preprocessed): 0.00\n",
      "Levenshtein(fully preprocessed) : 0.45\n",
      "Jaccard (partially preprocessed): 0.20\n",
      "Dice (partially preprocessed): 0.07\n",
      "Levenshtein (partially preprocessed): 0.40\n",
      "\n",
      "Jaccard (fully preprocessed): 0.07\n",
      "Dice (fully preprocessed): 0.02\n",
      "Levenshtein(fully preprocessed) : 0.32\n",
      "Jaccard (partially preprocessed): 0.09\n",
      "Dice (partially preprocessed): 0.02\n",
      "Levenshtein (partially preprocessed): 0.32\n",
      "\n",
      "Jaccard (fully preprocessed): 0.48\n",
      "Dice (fully preprocessed): 0.50\n",
      "Levenshtein(fully preprocessed) : 0.70\n",
      "Jaccard (partially preprocessed): 0.57\n",
      "Dice (partially preprocessed): 0.61\n",
      "Levenshtein (partially preprocessed): 0.71\n",
      "\n",
      "Overall averages\n",
      "Jaccard (fully preprocessed): 0.24\n",
      "Dice (fully preprocessed): 0.23\n",
      "Levenshtein (fully preprocessed): 0.48\n",
      "Jaccard (partial preprocessed): 0.30\n",
      "Dice (partial preprocessed): 0.28\n",
      "Levenshtein (partial preprocessed): 0.50\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import  euclidean\n",
    "from scipy.stats import wasserstein_distance\n",
    "from Levenshtein import distance as levenshtein_distance\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk import ngrams\n",
    "from itertools import combinations\n",
    "\n",
    "\n",
    "# Define similarity measures\n",
    "def jaccard_similarity(str1, str2):\n",
    "    a = set(str1.split())\n",
    "    b = set(str2.split())\n",
    "    intersection = len(a.intersection(b))\n",
    "    union = len(a.union(b))\n",
    "    return intersection / union\n",
    "\n",
    "def dice_similarity(str1, str2):\n",
    "    a = set(ngrams(str1.split(), 2))\n",
    "    b = set(ngrams(str2.split(), 2))\n",
    "    intersection = len(a.intersection(b))\n",
    "    return 2 * intersection / (len(a) + len(b))\n",
    "\n",
    "\n",
    "\n",
    "def levenshtein_similarity(str1, str2):\n",
    "    max_len = max(len(str1), len(str2))\n",
    "    return 1 - (levenshtein_distance(str1, str2) / max_len)\n",
    "\n",
    "def compare_distances(df):\n",
    "    threshold = 0.50 # Using the threshold suggested by the paper\n",
    "    similar_pairs = []\n",
    "\n",
    "    for (index1, row1), (index2, row2) in combinations(df.iterrows(), 2):\n",
    "        if index1 == index2:\n",
    "            continue\n",
    "        tweet1 = row1['tweet_text_fpp']\n",
    "        tweet2 = row2['tweet_text_fpp']\n",
    "        tweet1_pp = row1['tweet_text_ppp']\n",
    "        tweet2_pp = row2['tweet_text_ppp']\n",
    "\n",
    "        if len(tweet1) == 0 or len(tweet2) == 0 or len(tweet1_pp) == 0 or len(tweet2_pp) == 0:\n",
    "            continue\n",
    "        try:\n",
    "            jaccard_sim = jaccard_similarity(tweet1, tweet2)\n",
    "            dice_sim = dice_similarity(tweet1, tweet2)\n",
    "\n",
    "            levenshtein_sim = levenshtein_similarity(tweet1, tweet2)\n",
    "            jaccard_sim_pp = jaccard_similarity(tweet1_pp, tweet2_pp)\n",
    "            dice_sim_pp = dice_similarity(tweet1_pp, tweet2_pp)\n",
    "\n",
    "            levenshtein_sim_pp = levenshtein_similarity(tweet1_pp, tweet2_pp)\n",
    "        except Exception as e:\n",
    "            #print(f\"Error comparing {tweet1} and {tweet2}: {e}\")\n",
    "            continue\n",
    "        \n",
    "\n",
    "        #if any distance is above the threshold, add the pair and all scores to the list\n",
    "\n",
    "        if  levenshtein_sim > threshold or levenshtein_sim_pp > threshold and levenshtein_sim < 1.0:\n",
    "            \n",
    "            similar_pairs.append({\n",
    "                'index1': index1,\n",
    "                'index2': index2,\n",
    "                'og_tweet1': row1['tweet_text'],\n",
    "                'og_tweet2': row2['tweet_text'],\n",
    "                'tweet1': tweet1,\n",
    "                'tweet2': tweet2,\n",
    "                'tweet1_pp': tweet1_pp,\n",
    "                'tweet2_pp': tweet2_pp,\n",
    "                'jaccard': jaccard_sim,\n",
    "                'dice': dice_sim,\n",
    "                #'wasserstein': wasserstein_sim,\n",
    "                'levenshtein': levenshtein_sim,\n",
    "                'jaccard_pp': jaccard_sim_pp,\n",
    "                'dice_pp': dice_sim_pp,\n",
    "                #'wasserstein_pp': wasserstein_sim_pp,\n",
    "                'levenshtein_pp': levenshtein_sim_pp\n",
    "            })\n",
    "\n",
    "\n",
    "    return similar_pairs\n",
    "\n",
    "#load the preprocessed dataset\n",
    "df_info_ops = pd.read_csv(file_path_info_ops.replace('.csv', '_preprocessed.csv'))\n",
    "\n",
    "#filter df_info_ops to those with tweet_text_pp and tweet_text of length > 30\n",
    "df_info_ops = df_info_ops[df_info_ops['tweet_text'].str.len() > 10]\n",
    "df_info_ops = df_info_ops[df_info_ops['tweet_text_fpp'].str.len() > 10]\n",
    "\n",
    "df_info_ops = df_info_ops[df_info_ops['tweet_text_ppp'].str.len() > 10]\n",
    "    \n",
    "# grab the first 1000 rows from each \n",
    "\n",
    "#filter df_info_ops to just english\n",
    "df_info_ops_en = df_info_ops[df_info_ops['tweet_language'] == 'en']\n",
    "\n",
    "#split info_ops into a list of dfs by whatever is the source column aand grab the first 1000 rows of each\n",
    "df_info_ops_en_list = [df_info_ops_en[df_info_ops_en['source'] == source].head(1000) for source in df_info_ops_en['source'].unique()]\n",
    "\n",
    "#compare distances for each source, for both the original and preprocessed text\n",
    "similars = []\n",
    "for sampled_df in df_info_ops_en_list:\n",
    "    #print the source\n",
    "    print(sampled_df['source'].iloc[0])\n",
    "    similars.append( compare_distances(sampled_df))\n",
    "\n",
    "#save the results to a json\n",
    "import json\n",
    "with open('similar_tweets.json', 'w') as f:\n",
    "    json.dump(similars, f, indent=4)\n",
    "\n",
    "#load the results from the json\n",
    "# with open('similar_tweets.json', 'r') as f:\n",
    "#     similars = json.load(f)\n",
    "\n",
    "#analyze average similarity scores\n",
    "def analyze_similarities(similars):\n",
    "    for source_similars in similars:\n",
    "        if len(source_similars) == 0:\n",
    "            continue\n",
    "        jaccard = np.mean([sim['jaccard'] for sim in source_similars])\n",
    "        dice = np.mean([sim['dice'] for sim in source_similars])\n",
    "        #wasserstein = np.mean([sim['wasserstein'] for sim in source_similars])\n",
    "        levenshtein = np.mean([sim['levenshtein'] for sim in source_similars])\n",
    "        jaccard_pp = np.mean([sim['jaccard_pp'] for sim in source_similars])\n",
    "        dice_pp = np.mean([sim['dice_pp'] for sim in source_similars])\n",
    "        #wasserstein_pp = np.mean([sim['wasserstein_pp'] for sim in source_similars])\n",
    "        levenshtein_pp = np.mean([sim['levenshtein_pp'] for sim in source_similars])\n",
    "        print(f\"Jaccard (fully preprocessed): {jaccard:.2f}\")\n",
    "        print(f\"Dice (fully preprocessed): {dice:.2f}\")\n",
    "        #print(f\"Wasserstein: {wasserstein:.2f}\")\n",
    "        print(f\"Levenshtein(fully preprocessed) : {levenshtein:.2f}\")\n",
    "        print(f\"Jaccard (partially preprocessed): {jaccard_pp:.2f}\")\n",
    "        print(f\"Dice (partially preprocessed): {dice_pp:.2f}\")\n",
    "        #print(f\"Wasserstein (preprocessed): {wasserstein_pp:.2f}\")\n",
    "        print(f\"Levenshtein (partially preprocessed): {levenshtein_pp:.2f}\")\n",
    "        print()\n",
    "    #print the overall average similarity scores\n",
    "    jaccard = np.mean([sim['jaccard'] for source_similars in similars for sim in source_similars])\n",
    "    dice = np.mean([sim['dice'] for source_similars in similars for sim in source_similars])\n",
    "    #wasserstein = np.mean([sim['wasserstein'] for source_similars in similars for sim in source_similars])\n",
    "    levenshtein = np.mean([sim['levenshtein'] for source_similars in similars for sim in source_similars])\n",
    "    jaccard_pp = np.mean([sim['jaccard_pp'] for source_similars in similars for sim in source_similars])\n",
    "    dice_pp = np.mean([sim['dice_pp'] for source_similars in similars for sim in source_similars])\n",
    "    #wasserstein_pp = np.mean([sim['wasserstein_pp'] for source_similars in similars for sim in source_similars])\n",
    "    levenshtein_pp = np.mean([sim['levenshtein_pp'] for source_similars in similars for sim in source_similars])\n",
    "    print(\"Overall averages\")\n",
    "    print(f\"Jaccard (fully preprocessed): {jaccard:.2f}\")\n",
    "    print(f\"Dice (fully preprocessed): {dice:.2f}\")\n",
    "    #print(f\"Wasserstein: {wasserstein:.2f}\")\n",
    "    print(f\"Levenshtein (fully preprocessed): {levenshtein:.2f}\")\n",
    "    print(f\"Jaccard (partial preprocessed): {jaccard_pp:.2f}\")\n",
    "    print(f\"Dice (partial preprocessed): {dice_pp:.2f}\")\n",
    "    #print(f\"Wasserstein (preprocessed): {wasserstein_pp:.2f}\")\n",
    "    print(f\"Levenshtein (partial preprocessed): {levenshtein_pp:.2f}\")\n",
    "    \n",
    "\n",
    "#analyze the similarities\n",
    "analyze_similarities(similars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking USE, BERT, SBERT, RoBERTa, for shortform translated content\n",
    "\n",
    "Rather than generating a synthentic dataset, as the original paper does, I use another source of translated shortform content to benchmark models for use in the translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correctly identified 65233 out of 69487 nearest neighbors.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('C:\\\\Users\\\\benzo\\\\repo\\\\nlp-project\\\\data\\\\gemma\\\\70k_gemma_template_built.csv')\n",
    "df = df[['original_text', 'generated_text']]  # Filter out unnecessary columns\n",
    "\n",
    "\n",
    "# Generate unique IDs\n",
    "df['id'] = np.arange(len(df))\n",
    "\n",
    "# Text Embedding\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "all_text = pd.concat([df['original_text'], df['generated_text']])\n",
    "tfidf_matrix = vectorizer.fit_transform(all_text)\n",
    "tfidf_matrix_normalized = normalize(tfidf_matrix)\n",
    "\n",
    "\n",
    "# Split the matrices\n",
    "midpoint = len(df)\n",
    "original_text_matrix = tfidf_matrix_normalized[:midpoint]\n",
    "generated_text_matrix = tfidf_matrix_normalized[midpoint:]\n",
    "\n",
    "# Apply KNN to find the nearest rewritten text for each original text\n",
    "knn = NearestNeighbors(n_neighbors=1, metric='cosine')\n",
    "knn.fit(generated_text_matrix)\n",
    "\n",
    "# Query the model to find the nearest neighbors\n",
    "distances, indices = knn.kneighbors(original_text_matrix)\n",
    "\n",
    "# Map indices to IDs\n",
    "df['nearest_rewritten_id'] = df.iloc[indices.flatten()]['id'].values\n",
    "df['nearest_rewritten_text'] = df.iloc[indices.flatten()]['generated_text'].values\n",
    "\n",
    "# Output results -- how many nearest neighbors are correctly identified as having the same id\n",
    "correct = np.sum(df['id'] == df['nearest_rewritten_id'])\n",
    "total = len(df)\n",
    "print(f\"TF - IDF - Correctly identified {correct} out of {total} nearest neighbors, {correct/total*100}%\")\n",
    "\n",
    "# redo with bert embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\benzo\\repo\\nlp-project\\final-project.ipynb Cell 23\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/benzo/repo/nlp-project/final-project.ipynb#X45sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m# Generate embeddings\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/benzo/repo/nlp-project/final-project.ipynb#X45sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m all_text \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([df[\u001b[39m'\u001b[39m\u001b[39moriginal_text\u001b[39m\u001b[39m'\u001b[39m], df[\u001b[39m'\u001b[39m\u001b[39mgenerated_text\u001b[39m\u001b[39m'\u001b[39m]])\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/benzo/repo/nlp-project/final-project.ipynb#X45sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m all_embeddings \u001b[39m=\u001b[39m get_bert_embeddings(all_text)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/benzo/repo/nlp-project/final-project.ipynb#X45sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m# Split the embeddings\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/benzo/repo/nlp-project/final-project.ipynb#X45sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m midpoint \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(df)\n",
      "\u001b[1;32mc:\\Users\\benzo\\repo\\nlp-project\\final-project.ipynb Cell 23\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/benzo/repo/nlp-project/final-project.ipynb#X45sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/benzo/repo/nlp-project/final-project.ipynb#X45sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     inputs \u001b[39m=\u001b[39m tokenizer(text, return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m, padding\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, max_length\u001b[39m=\u001b[39m\u001b[39m512\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/benzo/repo/nlp-project/final-project.ipynb#X45sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/benzo/repo/nlp-project/final-project.ipynb#X45sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     \u001b[39m# Take the mean of all token embeddings to get a single vector per text\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/benzo/repo/nlp-project/final-project.ipynb#X45sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     mean_embedding \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mlast_hidden_state\u001b[39m.\u001b[39mmean(\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1013\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1004\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m   1006\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[0;32m   1007\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[0;32m   1008\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1011\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[0;32m   1012\u001b[0m )\n\u001b[1;32m-> 1013\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[0;32m   1014\u001b[0m     embedding_output,\n\u001b[0;32m   1015\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[0;32m   1016\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   1017\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m   1018\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[0;32m   1019\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m   1020\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   1021\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1022\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1023\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1024\u001b[0m )\n\u001b[0;32m   1025\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1026\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\models\\bert\\modeling_bert.py:607\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    596\u001b[0m     layer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    597\u001b[0m         layer_module\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m,\n\u001b[0;32m    598\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    604\u001b[0m         output_attentions,\n\u001b[0;32m    605\u001b[0m     )\n\u001b[0;32m    606\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 607\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m    608\u001b[0m         hidden_states,\n\u001b[0;32m    609\u001b[0m         attention_mask,\n\u001b[0;32m    610\u001b[0m         layer_head_mask,\n\u001b[0;32m    611\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    612\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    613\u001b[0m         past_key_value,\n\u001b[0;32m    614\u001b[0m         output_attentions,\n\u001b[0;32m    615\u001b[0m     )\n\u001b[0;32m    617\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    618\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\models\\bert\\modeling_bert.py:497\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    486\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    487\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    494\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m    495\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    496\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 497\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[0;32m    498\u001b[0m         hidden_states,\n\u001b[0;32m    499\u001b[0m         attention_mask,\n\u001b[0;32m    500\u001b[0m         head_mask,\n\u001b[0;32m    501\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    502\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[0;32m    503\u001b[0m     )\n\u001b[0;32m    504\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    506\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\models\\bert\\modeling_bert.py:427\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    418\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    419\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    425\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    426\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m--> 427\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[0;32m    428\u001b[0m         hidden_states,\n\u001b[0;32m    429\u001b[0m         attention_mask,\n\u001b[0;32m    430\u001b[0m         head_mask,\n\u001b[0;32m    431\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    432\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    433\u001b[0m         past_key_value,\n\u001b[0;32m    434\u001b[0m         output_attentions,\n\u001b[0;32m    435\u001b[0m     )\n\u001b[0;32m    436\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[0;32m    437\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\models\\bert\\modeling_bert.py:355\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    352\u001b[0m     attention_scores \u001b[39m=\u001b[39m attention_scores \u001b[39m+\u001b[39m attention_mask\n\u001b[0;32m    354\u001b[0m \u001b[39m# Normalize the attention scores to probabilities.\u001b[39;00m\n\u001b[1;32m--> 355\u001b[0m attention_probs \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mfunctional\u001b[39m.\u001b[39;49msoftmax(attention_scores, dim\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m    357\u001b[0m \u001b[39m# This is actually dropping out entire tokens to attend to, which might\u001b[39;00m\n\u001b[0;32m    358\u001b[0m \u001b[39m# seem a bit unusual, but is taken from the original Transformer paper.\u001b[39;00m\n\u001b[0;32m    359\u001b[0m attention_probs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(attention_probs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\functional.py:1856\u001b[0m, in \u001b[0;36msoftmax\u001b[1;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[0;32m   1854\u001b[0m     dim \u001b[39m=\u001b[39m _get_softmax_dim(\u001b[39m\"\u001b[39m\u001b[39msoftmax\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim(), _stacklevel)\n\u001b[0;32m   1855\u001b[0m \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1856\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39minput\u001b[39;49m\u001b[39m.\u001b[39;49msoftmax(dim)\n\u001b[0;32m   1857\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1858\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msoftmax(dim, dtype\u001b[39m=\u001b[39mdtype)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# REDO Above with BERT embeddings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "# Generate unique IDs\n",
    "df['id'] = np.arange(len(df))\n",
    "\n",
    "df = df.head(100)\n",
    "\n",
    "# Load BERT model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = AutoModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def get_bert_embeddings(texts):\n",
    "    embeddings = []\n",
    "    for idx, text in texts:\n",
    "        with torch.no_grad():\n",
    "            print(f\"Processing text {idx}\")\n",
    "            inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "            outputs = model(**inputs)\n",
    "            # Take the mean of all token embeddings to get a single vector per text\n",
    "            mean_embedding = outputs.last_hidden_state.mean(1)\n",
    "            embeddings.append(mean_embedding.squeeze().numpy())\n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "# Generate embeddings\n",
    "all_text = pd.concat([df['original_text'], df['generated_text']])\n",
    "all_embeddings = get_bert_embeddings(all_text)\n",
    "\n",
    "# Split the embeddings\n",
    "midpoint = len(df)\n",
    "original_embeddings = all_embeddings[:midpoint]\n",
    "generated_embeddings = all_embeddings[midpoint:]\n",
    "\n",
    "# Apply KNN\n",
    "knn = NearestNeighbors(n_neighbors=1, metric='cosine')\n",
    "knn.fit(generated_embeddings)\n",
    "\n",
    "# Query the model\n",
    "distances, indices = knn.kneighbors(original_embeddings)\n",
    "\n",
    "# Map indices to IDs\n",
    "df['nearest_rewritten_id'] = df.iloc[indices.flatten()]['id'].values\n",
    "df['nearest_rewritten_text'] = df.iloc[indices.flatten()]['generated_text'].values\n",
    "\n",
    "# Evaluate results\n",
    "correct = np.sum(df['id'] == df['nearest_rewritten_id'])\n",
    "total = len(df)\n",
    "print(f\"BERT - Correctly identified {correct} out of {total} nearest neighbors, {correct/total*100}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvements —  testing predictivity\n",
    "\n",
    "The point-in-time nature of the original 3∆ allows only for testing prediction validity within the datasets  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvements —  image p-hashes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvements: Leveraging Retrieval-Augmented Generation (RAG) for Detecting Textual Manipulation\n",
    "The detection of textual manipulation on social media is a complex and evolving challenge. Traditional methods, such as the 3∆-space duplicate methodology, have made significant strides in identifying manipulated content through semantic, grapheme, and language proximity analysis. However, these methods have limitations, particularly in handling diverse languages, short text length, and evolving manipulation techniques. To address these limitations, we propose an alternative methodology leveraging Retrieval-Augmented Generation (RAG), a state-of-the-art technique that combines the strengths of retrieval-based and generation-based models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suggestions for future research\n",
    "\n",
    "Integrate context aware p-hashes or other near duplicate image detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Citations:\n",
    "https://aclanthology.org/2023.findings-acl.426.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://aclanthology.org/2023.findings-acl.426.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
